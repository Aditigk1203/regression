# -*- coding: utf-8 -*-
"""car_purchasing_mutiplereg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DVWi9egCcfUplFVBWzc4rSSEgzDUPL28

# DATA PRE PROCESSING
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler , LabelEncoder
import seaborn as sns
import matplotlib.pyplot as plt
# Load the dataset
df = pd.read_csv('/content/car_purchasing.csv',encoding='latin-1')

# EDA
df.drop(columns=['Benefits'], inplace=True)

print(df.info())
print(df.describe())
le=LabelEncoder()
numeric_data = df.select_dtypes(include=['float64', 'int64'])

correlation_matrix = numeric_data.corr()

sns.heatmap(correlation_matrix, annot=True)
plt.figure(figsize=(10, 6))
plt.show()



print(df.columns)
print (df.head(5))

df.drop(columns=['TotalPayBenefits','customername','customer e-mail'],inplace=True)
print(df.head(10))

numeric_data = df.select_dtypes(include=['float64', 'int64'])
correlation_matrix = numeric_data.corr()
sns.heatmap(correlation_matrix, annot=True)
plt.show()

df.drop(columns=['country'],inplace=True)

df.head(10)

df=pd.get_dummies(df,columns=['job_category'],drop_first=True)
df.drop(columns=['JobTitle'],inplace=True)

df.info()

from sklearn.preprocessing import StandardScaler
# Getting all numeric columnns to check if data is skewed
numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

print(numerical_columns)

scaler = StandardScaler()

scaled_data = scaler.fit_transform(df[numerical_columns])
print(df[numerical_columns].isna().sum())

skew_vals = df.select_dtypes(include=['float64', 'int64']).skew()
print(skew_vals[skew_vals > 1])  # Highly skewed

import numpy as np
df['TotalPay'] = np.log1p(df['TotalPay'])
df['OtherPay'] = np.cbrt(df['OtherPay'])
print(df.select_dtypes(include=['float64', 'int64']).skew())

y=df['car purchase amount']
X=df.drop(columns=['car purchase amount'])

from sklearn.model_selection import train_test_split
# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42 )

# Optional: Confirm the shapes
print("Training set shape:", X_train.shape)
print("Test set shape:", X_test.shape)

"""# MULTIPLE REG

SCIKIT LEARN
"""

from sklearn.linear_model import LinearRegression

# Initialize the model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)
print(y_pred)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Calculate MSE and R2
mae=mean_absolute_error(y_test, y_pred) #not sensitive to outliers unline MSE AND RMSE , lower thebetter
print("MAE",mae)
mse = mean_squared_error(y_test, y_pred) #non negetive always , lower the value , better
r2 = r2_score(y_test, y_pred) #proportion of the variance in the dependent variable which is explained by the linear regression model
print("MSE =" , mse , "   \nR2 =",r2)
rmse = np.sqrt(mse) #how well the predicted values from a model align with the actual observed values in the dataset
print("RMSE =", rmse)

"""this means that
MAE = almost 5000 , the models prediction is 5000 off from the true cost

MSE = 38513754 - avg squared prediction errors

R2 = 64% - model able to explain variance in cost

RMSE = 6205 , approx error is around this much
"""

print("Mean of y_test:", y_test.mean())
print("Standard deviation of y_test:", y_test.std())

"""STATS MODEL"""

df

df.info()

y=df['car purchase amount']
X=df.drop(columns=['car purchase amount'])

# Convert only boolean columns to int
X = X.astype({col: int for col in X.select_dtypes(include='bool').columns})

import statsmodels.api as sm
regression=sm.OLS(y,sm.add_constant(X))
results=regression.fit()
results.summary()

"""r2 = 63%
means that the current model captures 63% of the original variability in the training data




"""

print(X.dtypes)

"""# RANDOM FOREST"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

model=RandomForestRegressor(n_estimators=100,random_state=42)
model.fit(X_train,y_train)
y_pred=model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Random Forest Regression Results:")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R-squared (R2): {r2:.3f}")



import matplotlib.pyplot as plt

importances = model.feature_importances_
features = X.columns

plt.barh(features, importances)
plt.xlabel("Feature Importance")
plt.title("Random Forest Feature Importances")
plt.show()

"""# ANN"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam

model = Sequential([
    Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(1)
])


optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

# ==== Early stopping ====
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# ==== Train the model ====
history = model.fit(
    X_train_scaled, y_train,
    validation_split=0.2,
    epochs=100,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

# ==== Evaluate the model ====
y_pred = model.predict(X_test_scaled).flatten()
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R-squared (R2): {r2:.3f}")

# ==== Plot predictions vs actual ====
plt.figure(figsize=(6, 5))
plt.scatter(y_test, y_pred)
plt.xlabel("True Values")
plt.ylabel("Predicted Values")
plt.title("Predictions vs Actual")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')  # perfect prediction line
plt.grid(True)
plt.tight_layout()
plt.show()